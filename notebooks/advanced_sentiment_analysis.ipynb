{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/codespace/.python/current/lib/python3.12/site-packages (4.49.0)\n",
      "Requirement already satisfied: datasets in /home/codespace/.python/current/lib/python3.12/site-packages (3.3.2)\n",
      "Requirement already satisfied: evaluate in /home/codespace/.python/current/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: optuna in /home/codespace/.python/current/lib/python3.12/site-packages (4.2.1)\n",
      "Requirement already satisfied: shap in /home/codespace/.python/current/lib/python3.12/site-packages (0.47.0)\n",
      "Requirement already satisfied: xgboost in /home/codespace/.python/current/lib/python3.12/site-packages (2.1.4)\n",
      "Requirement already satisfied: lightgbm in /home/codespace/.python/current/lib/python3.12/site-packages (4.6.0)\n",
      "Requirement already satisfied: pandas-profiling in /home/codespace/.python/current/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: tqdm in /home/codespace/.python/current/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.python/current/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/codespace/.python/current/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/codespace/.python/current/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/codespace/.python/current/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/codespace/.python/current/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/codespace/.python/current/lib/python3.12/site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from optuna) (1.15.1)\n",
      "Requirement already satisfied: colorlog in /home/codespace/.python/current/lib/python3.12/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from optuna) (2.0.39)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.12/site-packages (from shap) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.python/current/lib/python3.12/site-packages (from shap) (1.3.2)\n",
      "Requirement already satisfied: slicer==0.0.8 in /home/codespace/.python/current/lib/python3.12/site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in /home/codespace/.python/current/lib/python3.12/site-packages (from shap) (0.61.0)\n",
      "Requirement already satisfied: cloudpickle in /home/codespace/.python/current/lib/python3.12/site-packages (from shap) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in /home/codespace/.local/lib/python3.12/site-packages (from shap) (4.12.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/codespace/.python/current/lib/python3.12/site-packages (from xgboost) (2.26.2)\n",
      "Requirement already satisfied: joblib~=1.1.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pandas-profiling) (1.1.1)\n",
      "Requirement already satisfied: matplotlib>=3.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from pandas-profiling) (3.9.3)\n",
      "Requirement already satisfied: pydantic>=1.8.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from pandas-profiling) (2.10.6)\n",
      "Requirement already satisfied: jinja2>=2.11.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas-profiling) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.1.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from pandas-profiling) (2.1.5)\n",
      "Collecting visions==0.7.4 (from visions[type_image_path]==0.7.4->pandas-profiling)\n",
      "  Using cached visions-0.7.4-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: htmlmin>=0.1.12 in /home/codespace/.python/current/lib/python3.12/site-packages (from pandas-profiling) (0.1.12)\n",
      "Requirement already satisfied: missingno>=0.4.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from pandas-profiling) (0.5.2)\n",
      "Requirement already satisfied: phik>=0.11.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from pandas-profiling) (0.12.4)\n",
      "Requirement already satisfied: tangled-up-in-unicode==0.2.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pandas-profiling) (0.2.0)\n",
      "Requirement already satisfied: seaborn>=0.10.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas-profiling) (0.13.2)\n",
      "Requirement already satisfied: multimethod>=1.4 in /home/codespace/.python/current/lib/python3.12/site-packages (from pandas-profiling) (1.12)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from visions==0.7.4->visions[type_image_path]==0.7.4->pandas-profiling) (24.2.0)\n",
      "Requirement already satisfied: networkx>=2.4 in /home/codespace/.local/lib/python3.12/site-packages (from visions==0.7.4->visions[type_image_path]==0.7.4->pandas-profiling) (3.2.1)\n",
      "Requirement already satisfied: imagehash in /home/codespace/.python/current/lib/python3.12/site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (4.3.1)\n",
      "Requirement already satisfied: Pillow in /home/codespace/.local/lib/python3.12/site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (11.0.0)\n",
      "Requirement already satisfied: Mako in /home/codespace/.python/current/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.2.0->pandas-profiling) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.2.0->pandas-profiling) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.2.0->pandas-profiling) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.2.0->pandas-profiling) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.2.0->pandas-profiling) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.2.0->pandas-profiling) (2.9.0.post0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/codespace/.python/current/lib/python3.12/site-packages (from numba>=0.54->shap) (0.44.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic>=1.8.1->pandas-profiling) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic>=1.8.1->pandas-profiling) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/codespace/.python/current/lib/python3.12/site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->shap) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.2.0->pandas-profiling) (1.17.0)\n",
      "Requirement already satisfied: PyWavelets in /home/codespace/.python/current/lib/python3.12/site-packages (from imagehash->visions[type_image_path]==0.7.4->pandas-profiling) (1.8.0)\n",
      "Using cached visions-0.7.4-py3-none-any.whl (102 kB)\n",
      "Installing collected packages: visions\n",
      "  Attempting uninstall: visions\n",
      "    Found existing installation: visions 0.8.1\n",
      "    Uninstalling visions-0.8.1:\n",
      "      Successfully uninstalled visions-0.8.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ydata-profiling 4.14.0 requires visions[type_image_path]<0.8.2,>=0.7.5, but you have visions 0.7.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed visions-0.7.4\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate optuna shap xgboost lightgbm pandas-profiling tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.3.2 in /home/codespace/.python/current/lib/python3.12/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /home/codespace/.python/current/lib/python3.12/site-packages (from scikit-learn==1.3.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn==1.3.2) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from scikit-learn==1.3.2) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn==1.3.2) (3.5.0)\n",
      "Requirement already satisfied: evaluate in /home/codespace/.python/current/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from evaluate) (3.3.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.python/current/lib/python3.12/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /home/codespace/.python/current/lib/python3.12/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/codespace/.local/lib/python3.12/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/codespace/.python/current/lib/python3.12/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/codespace/.python/current/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/codespace/.local/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from evaluate) (0.29.3)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in /home/codespace/.python/current/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.11.13)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==1.3.2\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import optuna\n",
    "# Replace this deprecated import\n",
    "# from datasets import load_metric\n",
    "# With the current approach from evaluate library\n",
    "import evaluate\n",
    "import shap\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device and seeds\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Loading and Preparation\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (example with mock data)\n",
    "df = pd.read_csv('/workspaces/Social-Media-Sentiment-Analysis-Trend-Forecasting/data/Tweets.csv')  # Replace with your dataset\n",
    "df = df.drop_duplicates().dropna()\n",
    "\n",
    "# Advanced text preprocessing\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'[\\U00010000-\\U0010ffff]', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text.lower()\n",
    "\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /home/codespace/.python/current/lib/python3.12/site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in /home/codespace/.python/current/lib/python3.12/site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in /home/codespace/.python/current/lib/python3.12/site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/codespace/.python/current/lib/python3.12/site-packages (from nltk>=3.9->textblob) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/codespace/.python/current/lib/python3.12/site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/codespace/.python/current/lib/python3.12/site-packages (from nltk>=3.9->textblob) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "from textblob import TextBlob\n",
    "# Feature engineering\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "df['num_hashtags'] = df['text'].apply(lambda x: len(re.findall(r'#\\w+', x)))\n",
    "df['sentiment_score'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ydata-profiling in /home/codespace/.python/current/lib/python3.12/site-packages (4.14.0)\n",
      "Requirement already satisfied: scipy<1.16,>=1.4.1 in /home/codespace/.local/lib/python3.12/site-packages (from ydata-profiling) (1.14.1)\n",
      "Requirement already satisfied: pandas!=1.4.0,<3.0,>1.1 in /home/codespace/.local/lib/python3.12/site-packages (from ydata-profiling) (2.2.3)\n",
      "Requirement already satisfied: matplotlib<=3.10,>=3.5 in /home/codespace/.local/lib/python3.12/site-packages (from ydata-profiling) (3.9.3)\n",
      "Requirement already satisfied: pydantic>=2 in /home/codespace/.python/current/lib/python3.12/site-packages (from ydata-profiling) (2.10.6)\n",
      "Requirement already satisfied: PyYAML<6.1,>=5.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from ydata-profiling) (6.0.2)\n",
      "Requirement already satisfied: jinja2<3.2,>=2.11.1 in /home/codespace/.local/lib/python3.12/site-packages (from ydata-profiling) (3.1.4)\n",
      "Collecting visions<0.8.2,>=0.7.5 (from visions[type_image_path]<0.8.2,>=0.7.5->ydata-profiling)\n",
      "  Using cached visions-0.8.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.2,>=1.16.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from ydata-profiling) (1.26.4)\n",
      "Requirement already satisfied: htmlmin==0.1.12 in /home/codespace/.python/current/lib/python3.12/site-packages (from ydata-profiling) (0.1.12)\n",
      "Requirement already satisfied: phik<0.13,>=0.11.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from ydata-profiling) (0.12.4)\n",
      "Requirement already satisfied: requests<3,>=2.24.0 in /home/codespace/.local/lib/python3.12/site-packages (from ydata-profiling) (2.32.3)\n",
      "Requirement already satisfied: tqdm<5,>=4.48.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from ydata-profiling) (4.67.1)\n",
      "Requirement already satisfied: seaborn<0.14,>=0.10.1 in /home/codespace/.local/lib/python3.12/site-packages (from ydata-profiling) (0.13.2)\n",
      "Requirement already satisfied: multimethod<2,>=1.4 in /home/codespace/.python/current/lib/python3.12/site-packages (from ydata-profiling) (1.12)\n",
      "Requirement already satisfied: statsmodels<1,>=0.13.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from ydata-profiling) (0.14.4)\n",
      "Requirement already satisfied: typeguard<5,>=3 in /home/codespace/.python/current/lib/python3.12/site-packages (from ydata-profiling) (4.4.2)\n",
      "Requirement already satisfied: imagehash==4.3.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from ydata-profiling) (4.3.1)\n",
      "Requirement already satisfied: wordcloud>=1.9.3 in /home/codespace/.python/current/lib/python3.12/site-packages (from ydata-profiling) (1.9.4)\n",
      "Requirement already satisfied: dacite>=1.8 in /home/codespace/.python/current/lib/python3.12/site-packages (from ydata-profiling) (1.9.2)\n",
      "Requirement already satisfied: numba<1,>=0.56.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from ydata-profiling) (0.61.0)\n",
      "Requirement already satisfied: PyWavelets in /home/codespace/.python/current/lib/python3.12/site-packages (from imagehash==4.3.1->ydata-profiling) (1.8.0)\n",
      "Requirement already satisfied: pillow in /home/codespace/.local/lib/python3.12/site-packages (from imagehash==4.3.1->ydata-profiling) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from jinja2<3.2,>=2.11.1->ydata-profiling) (2.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib<=3.10,>=3.5->ydata-profiling) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib<=3.10,>=3.5->ydata-profiling) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib<=3.10,>=3.5->ydata-profiling) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib<=3.10,>=3.5->ydata-profiling) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib<=3.10,>=3.5->ydata-profiling) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib<=3.10,>=3.5->ydata-profiling) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib<=3.10,>=3.5->ydata-profiling) (2.9.0.post0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/codespace/.python/current/lib/python3.12/site-packages (from numba<1,>=0.56.0->ydata-profiling) (0.44.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas!=1.4.0,<3.0,>1.1->ydata-profiling) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas!=1.4.0,<3.0,>1.1->ydata-profiling) (2024.2)\n",
      "Requirement already satisfied: joblib>=0.14.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from phik<0.13,>=0.11.1->ydata-profiling) (1.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic>=2->ydata-profiling) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic>=2->ydata-profiling) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic>=2->ydata-profiling) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.24.0->ydata-profiling) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.24.0->ydata-profiling) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.24.0->ydata-profiling) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.24.0->ydata-profiling) (2024.8.30)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /home/codespace/.python/current/lib/python3.12/site-packages (from statsmodels<1,>=0.13.2->ydata-profiling) (1.0.1)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from visions<0.8.2,>=0.7.5->visions[type_image_path]<0.8.2,>=0.7.5->ydata-profiling) (24.2.0)\n",
      "Requirement already satisfied: networkx>=2.4 in /home/codespace/.local/lib/python3.12/site-packages (from visions<0.8.2,>=0.7.5->visions[type_image_path]<0.8.2,>=0.7.5->ydata-profiling) (3.2.1)\n",
      "Requirement already satisfied: puremagic in /home/codespace/.python/current/lib/python3.12/site-packages (from visions<0.8.2,>=0.7.5->visions[type_image_path]<0.8.2,>=0.7.5->ydata-profiling) (1.28)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib<=3.10,>=3.5->ydata-profiling) (1.17.0)\n",
      "Using cached visions-0.8.1-py3-none-any.whl (105 kB)\n",
      "Installing collected packages: visions\n",
      "  Attempting uninstall: visions\n",
      "    Found existing installation: visions 0.7.4\n",
      "    Uninstalling visions-0.7.4:\n",
      "      Successfully uninstalled visions-0.7.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas-profiling 3.2.0 requires visions[type_image_path]==0.7.4, but you have visions 0.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed visions-0.8.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"ttps://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset: 100%|██████████| 28/28 [00:01<00:00, 15.49it/s, Completed]                                    \n",
      "Generate report structure:   0%|          | 0/1 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# or use: from pandas_profiling import ProfileReport  # older package name\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 2. Advanced EDA\u001b[39;00m\n\u001b[1;32m      7\u001b[0m profile \u001b[38;5;241m=\u001b[39m ProfileReport(df, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreliminary Data Analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mprofile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meda_report.html\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m     11\u001b[0m sns\u001b[38;5;241m.\u001b[39mhistplot(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_score\u001b[39m\u001b[38;5;124m'\u001b[39m], kde\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/ydata_profiling/profile_report.py:374\u001b[0m, in \u001b[0;36mProfileReport.to_file\u001b[0;34m(self, output_file, silent)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhtml\u001b[38;5;241m.\u001b[39massets_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(output_file\u001b[38;5;241m.\u001b[39mstem) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_assets\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m     create_html_assets(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, output_file)\n\u001b[0;32m--> 374\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_file\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    377\u001b[0m     suffix \u001b[38;5;241m=\u001b[39m output_file\u001b[38;5;241m.\u001b[39msuffix\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/ydata_profiling/profile_report.py:491\u001b[0m, in \u001b[0;36mProfileReport.to_html\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_html\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    484\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate and return complete template as lengthy string\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;124;03m        for using with frameworks.\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m \n\u001b[1;32m    490\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhtml\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/ydata_profiling/profile_report.py:287\u001b[0m, in \u001b[0;36mProfileReport.html\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhtml\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_html \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 287\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_html \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_html\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/ydata_profiling/profile_report.py:404\u001b[0m, in \u001b[0;36mProfileReport._render_html\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_render_html\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mydata_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreport\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpresentation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflavours\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTMLReport\n\u001b[0;32m--> 404\u001b[0m     report \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreport\u001b[49m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\n\u001b[1;32m    407\u001b[0m         total\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRender HTML\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mprogress_bar\n\u001b[1;32m    408\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m    409\u001b[0m         html \u001b[38;5;241m=\u001b[39m HTMLReport(copy\u001b[38;5;241m.\u001b[39mdeepcopy(report))\u001b[38;5;241m.\u001b[39mrender(\n\u001b[1;32m    410\u001b[0m             nav\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhtml\u001b[38;5;241m.\u001b[39mnavbar_show,\n\u001b[1;32m    411\u001b[0m             offline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhtml\u001b[38;5;241m.\u001b[39muse_local_assets,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    419\u001b[0m             version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription_set\u001b[38;5;241m.\u001b[39mpackage[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mydata_profiling_version\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    420\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/ydata_profiling/profile_report.py:281\u001b[0m, in \u001b[0;36mProfileReport.report\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreport\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Root:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report \u001b[38;5;241m=\u001b[39m \u001b[43mget_report_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescription_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/ydata_profiling/report/structure/report.py:387\u001b[0m, in \u001b[0;36mget_report_structure\u001b[0;34m(config, summary)\u001b[0m\n\u001b[1;32m    368\u001b[0m section_items: List[Renderable] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    369\u001b[0m     Container(\n\u001b[1;32m    370\u001b[0m         get_dataset_items(config, summary, alerts),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m     ),\n\u001b[1;32m    375\u001b[0m ]\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(summary\u001b[38;5;241m.\u001b[39mvariables) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    378\u001b[0m     section_items\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    379\u001b[0m         Dropdown(\n\u001b[1;32m    380\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariables\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    381\u001b[0m             anchor_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariables-dropdown\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    382\u001b[0m             \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariables-dropdown\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    383\u001b[0m             is_row\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    384\u001b[0m             classes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mform-select\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    385\u001b[0m             items\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(summary\u001b[38;5;241m.\u001b[39mvariables),\n\u001b[1;32m    386\u001b[0m             item\u001b[38;5;241m=\u001b[39mContainer(\n\u001b[0;32m--> 387\u001b[0m                 \u001b[43mrender_variables_section\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    388\u001b[0m                 sequence_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccordion\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    389\u001b[0m                 name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariables\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    390\u001b[0m                 anchor_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    391\u001b[0m             ),\n\u001b[1;32m    392\u001b[0m         )\n\u001b[1;32m    393\u001b[0m     )\n\u001b[1;32m    395\u001b[0m scatter_items \u001b[38;5;241m=\u001b[39m get_interactions(config, summary\u001b[38;5;241m.\u001b[39mscatter)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scatter_items) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/ydata_profiling/report/structure/report.py:162\u001b[0m, in \u001b[0;36mrender_variables_section\u001b[0;34m(config, dataframe_summary)\u001b[0m\n\u001b[1;32m    160\u001b[0m     variable_type \u001b[38;5;241m=\u001b[39m summary[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    161\u001b[0m render_map_type \u001b[38;5;241m=\u001b[39m render_map\u001b[38;5;241m.\u001b[39mget(variable_type, render_map[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 162\u001b[0m template_variables\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mrender_map_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_variables\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Ignore these\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reject_variables:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/ydata_profiling/report/structure/variables/render_text.py:94\u001b[0m, in \u001b[0;36mrender_text\u001b[0;34m(config, summary)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# length isn't being computed for categorical in spark\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m summary:\n\u001b[0;32m---> 94\u001b[0m     length_table, length_histo \u001b[38;5;241m=\u001b[39m \u001b[43mrender_categorical_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvarid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     overview_items\u001b[38;5;241m.\u001b[39mappend(length_table)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# characters isn't being computed for categorical in spark\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/ydata_profiling/report/structure/variables/render_categorical.py:96\u001b[0m, in \u001b[0;36mrender_categorical_length\u001b[0;34m(config, summary, varid)\u001b[0m\n\u001b[1;32m     90\u001b[0m     hist_data \u001b[38;5;241m=\u001b[39m histogram(\n\u001b[1;32m     91\u001b[0m         config,\n\u001b[1;32m     92\u001b[0m         [x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m summary[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistogram_length\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[1;32m     93\u001b[0m         [x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m summary[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistogram_length\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[1;32m     94\u001b[0m     )\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     hist_data \u001b[38;5;241m=\u001b[39m \u001b[43mhistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhistogram_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m length_histo \u001b[38;5;241m=\u001b[39m Image(\n\u001b[1;32m     99\u001b[0m     hist_data,\n\u001b[1;32m    100\u001b[0m     image_format\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mimage_format,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m     anchor_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvarid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m length_table, length_histo\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/ydata_profiling/visualisation/plot.py:156\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(config, series, bins, date)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;129m@manage_matplotlib_context\u001b[39m()\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhistogram\u001b[39m(\n\u001b[1;32m    139\u001b[0m     config: Settings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m     date: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    143\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Plot an histogram of the data.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     plot \u001b[38;5;241m=\u001b[39m \u001b[43m_plot_histogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     plot\u001b[38;5;241m.\u001b[39mxaxis\u001b[38;5;241m.\u001b[39mset_tick_params(rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m date \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m45\u001b[39m)\n\u001b[1;32m    158\u001b[0m     plot\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mtight_layout()\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/ydata_profiling/visualisation/plot.py:108\u001b[0m, in \u001b[0;36m_plot_histogram\u001b[0;34m(config, series, bins, figsize, date, hide_yaxis)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39mfigsize)\n\u001b[0;32m--> 108\u001b[0m     plot \u001b[38;5;241m=\u001b[39m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_subplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m111\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hide_yaxis:\n\u001b[1;32m    110\u001b[0m         plot\u001b[38;5;241m.\u001b[39mset_ylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/matplotlib/figure.py:710\u001b[0m, in \u001b[0;36mFigureBase.add_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    708\u001b[0m         args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m(args[\u001b[38;5;241m0\u001b[39m])))\n\u001b[1;32m    709\u001b[0m     projection_class, pkw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_projection_requirements(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 710\u001b[0m     ax \u001b[38;5;241m=\u001b[39m \u001b[43mprojection_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    711\u001b[0m     key \u001b[38;5;241m=\u001b[39m (projection_class, pkw)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_axes_internal(ax, key)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/matplotlib/axes/_base.py:717\u001b[0m, in \u001b[0;36m_AxesBase.__init__\u001b[0;34m(self, fig, facecolor, frameon, sharex, sharey, label, xscale, yscale, box_aspect, forward_navigation_events, *args, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m rcParams \u001b[38;5;241m=\u001b[39m mpl\u001b[38;5;241m.\u001b[39mrcParams\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtick_params(\n\u001b[1;32m    703\u001b[0m     top\u001b[38;5;241m=\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxtick.top\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m rcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxtick.minor.top\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    704\u001b[0m     bottom\u001b[38;5;241m=\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxtick.bottom\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m rcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxtick.minor.bottom\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    714\u001b[0m                 rcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mytick.minor.right\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m    715\u001b[0m     which\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminor\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 717\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxtick.top\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxtick.major.top\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbottom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxtick.bottom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxtick.major.bottom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabeltop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxtick.labeltop\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\n\u001b[1;32m    721\u001b[0m \u001b[43m              \u001b[49m\u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxtick.major.top\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabelbottom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxtick.labelbottom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\n\u001b[1;32m    723\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxtick.major.bottom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mytick.left\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mytick.major.left\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mytick.right\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mytick.major.right\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabelleft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mytick.labelleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\n\u001b[1;32m    727\u001b[0m \u001b[43m               \u001b[49m\u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mytick.major.left\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabelright\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mytick.labelright\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\n\u001b[1;32m    729\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mytick.major.right\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhich\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmajor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/matplotlib/axes/_base.py:3464\u001b[0m, in \u001b[0;36m_AxesBase.tick_params\u001b[0;34m(self, axis, **kwargs)\u001b[0m\n\u001b[1;32m   3462\u001b[0m     xkw\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabelleft\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   3463\u001b[0m     xkw\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabelright\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 3464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxaxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_tick_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   3466\u001b[0m     ykw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/matplotlib/axis.py:1000\u001b[0m, in \u001b[0;36mAxis.set_tick_params\u001b[0;34m(self, which, reset, **kwargs)\u001b[0m\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_major_tick_kw\u001b[38;5;241m.\u001b[39mupdate(kwtrans)\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmajorTicks:\n\u001b[0;32m-> 1000\u001b[0m         \u001b[43mtick\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwtrans\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m which \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_minor_tick_kw\u001b[38;5;241m.\u001b[39mupdate(kwtrans)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/matplotlib/axis.py:357\u001b[0m, in \u001b[0;36mTick._apply_params\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    356\u001b[0m     tick_kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarkeredgecolor\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 357\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick1line\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtick_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtick2line\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtick_kw)\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tick_kw\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/matplotlib/artist.py:147\u001b[0m, in \u001b[0;36mArtist.__init_subclass__.<locals>.<lambda>\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_autogenerated_signature\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# Don't overwrite cls.set if the subclass or one of its parents\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# has defined a set method set itself.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# If there was no explicit definition, cls.set is inherited from\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# the hierarchy of auto-generated set methods, which hold the\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# flag _autogenerated_signature.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mArtist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/matplotlib/artist.py:1224\u001b[0m, in \u001b[0;36mArtist.set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1221\u001b[0m     \u001b[38;5;66;03m# docstring and signature are auto-generated via\u001b[39;00m\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;66;03m# Artist._update_set_signature_and_docstring() at the end of the\u001b[39;00m\n\u001b[1;32m   1223\u001b[0m     \u001b[38;5;66;03m# module.\u001b[39;00m\n\u001b[0;32m-> 1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_update(\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/matplotlib/cbook.py:1834\u001b[0m, in \u001b[0;36mnormalize_kwargs\u001b[0;34m(kw, alias_mapping)\u001b[0m\n\u001b[1;32m   1830\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(alias_mapping, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(alias_mapping, Artist)\n\u001b[1;32m   1831\u001b[0m       \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(alias_mapping, Artist)):\n\u001b[1;32m   1832\u001b[0m     alias_mapping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(alias_mapping, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_alias_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m-> 1834\u001b[0m to_canonical \u001b[38;5;241m=\u001b[39m {alias: canonical\n\u001b[1;32m   1835\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m canonical, alias_list \u001b[38;5;129;01min\u001b[39;00m alias_mapping\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1836\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m alias \u001b[38;5;129;01min\u001b[39;00m alias_list}\n\u001b[1;32m   1837\u001b[0m canonical_to_seen \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1838\u001b[0m ret \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# output dictionary\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!pip install ydata-profiling\n",
    "# Import the ProfileReport class\n",
    "from ydata_profiling import ProfileReport  # newer package name\n",
    "# or use: from pandas_profiling import ProfileReport  # older package name\n",
    "\n",
    "# 2. Advanced EDA\n",
    "profile = ProfileReport(df, title=\"Preliminary Data Analysis\")\n",
    "profile.to_file(\"eda_report.html\")\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.histplot(df['sentiment_score'], kde=True)\n",
    "plt.title('Sentiment Score Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',\n",
       "       'negativereason', 'negativereason_confidence', 'airline',\n",
       "       'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
       "       'retweet_count', 'text', 'tweet_coord', 'tweet_created',\n",
       "       'tweet_location', 'user_timezone', 'clean_text', 'text_length',\n",
       "       'num_hashtags', 'sentiment_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sentiment values: ['negative']\n"
     ]
    }
   ],
   "source": [
    "# First, define the TweetDataset class\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 3. Model Configuration\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3).to(device)\n",
    "\n",
    "# Use the correct column names from your DataFrame\n",
    "sentiment_column = 'airline_sentiment'  # This is the actual sentiment column in your DataFrame\n",
    "text_column = 'clean_text'  # This column exists in your DataFrame\n",
    "\n",
    "# Convert sentiment labels to numerical values\n",
    "# First, check unique values in sentiment column\n",
    "print(\"Unique sentiment values:\", df[sentiment_column].unique())\n",
    "\n",
    "# Create a mapping dictionary for sentiment labels\n",
    "sentiment_mapping = {\n",
    "    'negative': 0,\n",
    "    'neutral': 1,\n",
    "    'positive': 2\n",
    "}\n",
    "\n",
    "# Apply mapping to create a numerical label column\n",
    "df['sentiment_label'] = df[sentiment_column].map(sentiment_mapping)\n",
    "\n",
    "# Now split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df[text_column], \n",
    "    df['sentiment_label'], \n",
    "    test_size=0.2, \n",
    "    stratify=df['sentiment_label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = TweetDataset(X_train.tolist(), y_train.tolist(), tokenizer, 128)\n",
    "val_dataset = TweetDataset(X_val.tolist(), y_val.tolist(), tokenizer, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Hyperparameter Optimization with Optuna\n",
    "\n",
    "# First, create a compute_metrics function that includes F1 score\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Load metrics from evaluate\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    \n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    # Compute F1 score with macro averaging for multi-class\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='macro')\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"f1\": f1[\"f1\"]\n",
    "    }\n",
    "\n",
    "def objective(trial):\n",
    "    args = TrainingArguments(\n",
    "        output_dir='temp/',\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 1e-5, 5e-5),\n",
    "        per_device_train_batch_size=trial.suggest_categorical(\"batch_size\", [16, 32]),\n",
    "        num_train_epochs=trial.suggest_int(\"epochs\", 3, 6),\n",
    "        weight_decay=trial.suggest_float(\"weight_decay\", 0.0, 0.3),\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='f1',\n",
    "        push_to_hub=False,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10\n",
    "    )\n",
    "    # Create a new model instance for each trial to avoid carrying over state\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3).to(device)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,  # Add compute_metrics function\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    \n",
    "    # Return the F1 score (which will now be available thanks to compute_metrics)\n",
    "    return metrics['eval_f1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in /home/codespace/.python/current/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: transformers[torch] in /home/codespace/.python/current/lib/python3.12/site-packages (4.49.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/codespace/.python/current/lib/python3.12/site-packages (from accelerate>=0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from accelerate>=0.26.0) (24.2)\n",
      "Requirement already satisfied: psutil in /home/codespace/.local/lib/python3.12/site-packages (from accelerate>=0.26.0) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in /home/codespace/.local/lib/python3.12/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from accelerate>=0.26.0) (2.5.1+cpu)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.29.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/codespace/.python/current/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/codespace/.python/current/lib/python3.12/site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/codespace/.python/current/lib/python3.12/site-packages (from transformers[torch]) (0.21.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/codespace/.python/current/lib/python3.12/site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers[torch]) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers[torch]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers[torch]) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install 'accelerate>=0.26.0' transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 02:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.973809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.877760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model metrics: {'eval_loss': 0.8777599334716797, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_runtime': 1.2378, 'eval_samples_per_second': 0.808, 'eval_steps_per_second': 0.808, 'epoch': 3.0}\n",
      "Model saved to 'final_sentiment_model' directory\n"
     ]
    }
   ],
   "source": [
    "# First, define the compute_metrics function that includes F1 score\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Load metrics from evaluate\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    \n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    # Compute F1 score with macro averaging for multi-class\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='macro')\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"f1\": f1[\"f1\"]\n",
    "    }\n",
    "\n",
    "# After the installation, let's use a fixed set of hyperparameters\n",
    "# rather than using Optuna (which would require a kernel restart)\n",
    "learning_rate = 3e-5\n",
    "batch_size = 16\n",
    "epochs = 3\n",
    "weight_decay = 0.1\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Train a model with the manually set hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='best_model/',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    push_to_hub=False,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "# Create a new model instance\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3).to(device)\n",
    "\n",
    "# Create a trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating model...\")\n",
    "metrics = trainer.evaluate()\n",
    "print(f\"Model metrics: {metrics}\")\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"final_sentiment_model\")\n",
    "print(\"Model saved to 'final_sentiment_model' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentiment_column' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Apply mapping to create a numerical label column\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[43msentiment_column\u001b[49m]\u001b[38;5;241m.\u001b[39mmap(sentiment_mapping)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Split the data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentiment_column' is not defined"
     ]
    }
   ],
   "source": [
    "# Apply mapping to create a numerical label column\n",
    "df['sentiment_label'] = df[sentiment_column].map(sentiment_mapping)\n",
    "\n",
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Now split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df[text_column], \n",
    "    df['sentiment_label'], \n",
    "    test_size=0.2, \n",
    "    stratify=df['sentiment_label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Data split complete:\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Class distribution in training: {y_train.value_counts().to_dict()}\")\n",
    "# 5. Alternative Approach: Memory-Efficient Models\n",
    "print(\"Using memory-efficient models instead of BERT...\")\n",
    "\n",
    "# 5.1 TF-IDF + Linear Model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Create TF-IDF + Logistic Regression pipeline\n",
    "print(\"Training TF-IDF + Logistic Regression model...\")\n",
    "tfidf_model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1, 2))),\n",
    "    ('classifier', LogisticRegression(C=1, max_iter=1000, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "tfidf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "tfidf_preds = tfidf_model.predict(X_val)\n",
    "tfidf_probs = tfidf_model.predict_proba(X_val)\n",
    "\n",
    "# Calculate metrics\n",
    "tfidf_accuracy = accuracy_score(y_val, tfidf_preds)\n",
    "tfidf_f1 = f1_score(y_val, tfidf_preds, average='macro')\n",
    "\n",
    "print(f\"TF-IDF Model Accuracy: {tfidf_accuracy:.4f}\")\n",
    "print(f\"TF-IDF Model Macro F1: {tfidf_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, tfidf_preds, target_names=['Negative', 'Neutral', 'Positive']))\n",
    "\n",
    "# 5.2 Gradient Boosting Model\n",
    "print(\"\\nTraining LightGBM model...\")\n",
    "import lightgbm as lgb\n",
    "\n",
    "# First, create TF-IDF features\n",
    "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf.transform(X_val)\n",
    "\n",
    "# Convert to LightGBM dataset format\n",
    "lgb_train = lgb.Dataset(X_train_tfidf, y_train)\n",
    "lgb_val = lgb.Dataset(X_val_tfidf, y_val, reference=lgb_train)\n",
    "\n",
    "# LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 3,\n",
    "    'metric': 'multi_logloss',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Train LightGBM model\n",
    "print(\"Training LightGBM model...\")\n",
    "gbm_model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    num_boost_round=500,\n",
    "    valid_sets=[lgb_val],\n",
    "    early_stopping_rounds=20,\n",
    "    verbose_eval=100\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "gbm_preds = gbm_model.predict(X_val_tfidf)\n",
    "gbm_preds_labels = gbm_preds.argmax(axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "gbm_accuracy = accuracy_score(y_val, gbm_preds_labels)\n",
    "gbm_f1 = f1_score(y_val, gbm_preds_labels, average='macro')\n",
    "\n",
    "print(f\"LightGBM Model Accuracy: {gbm_accuracy:.4f}\")\n",
    "print(f\"LightGBM Model Macro F1: {gbm_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, gbm_preds_labels, target_names=['Negative', 'Neutral', 'Positive']))\n",
    "\n",
    "# 5.3 Feature Importance Analysis\n",
    "print(\"\\nAnalyzing feature importance...\")\n",
    "\n",
    "# Get top features from LightGBM\n",
    "feature_importance = gbm_model.feature_importance()\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_idx = feature_importance.argsort()[::-1]\n",
    "top_n = 20\n",
    "\n",
    "# Display top N important features\n",
    "print(f\"\\nTop {top_n} important features:\")\n",
    "for i in range(min(top_n, len(sorted_idx))):\n",
    "    print(f\"{feature_names[sorted_idx[i]]}: {feature_importance[sorted_idx[i]]}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(top_n), feature_importance[sorted_idx][:top_n])\n",
    "plt.yticks(range(top_n), [feature_names[i] for i in sorted_idx[:top_n]])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top Features for Sentiment Classification')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.show()\n",
    "\n",
    "# 5.4 Save the models\n",
    "import pickle\n",
    "\n",
    "print(\"\\nSaving models...\")\n",
    "with open('tfidf_model.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_model, f)\n",
    "    \n",
    "gbm_model.save_model('lightgbm_model.txt')\n",
    "\n",
    "print(\"Models saved successfully!\")\n",
    "\n",
    "# 5.5 Metrics comparison\n",
    "models = ['TF-IDF+LogReg', 'LightGBM']\n",
    "accuracy_scores = [tfidf_accuracy, gbm_accuracy]\n",
    "f1_scores = [tfidf_f1, gbm_f1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = range(len(models))\n",
    "width = 0.35\n",
    "plt.bar(x, accuracy_scores, width, label='Accuracy')\n",
    "plt.bar([i + width for i in x], f1_scores, width, label='F1 Score')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks([i + width/2 for i in x], models)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"Model Performance Summary:\")\n",
    "print(f\"TF-IDF+LogReg - Accuracy: {tfidf_accuracy:.4f}, F1: {tfidf_f1:.4f}\")\n",
    "print(f\"LightGBM - Accuracy: {gbm_accuracy:.4f}, F1: {gbm_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter optimization not found. Using default parameters.\n",
      "Default hyperparameters: {'learning_rate': 3e-05, 'batch_size': 16, 'epochs': 3, 'weight_decay': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final model...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 5. Final Model Training\n",
    "\n",
    "# Check if 'study' exists, otherwise use default parameters\n",
    "try:\n",
    "    best_params = study.best_params\n",
    "    print(\"Using optimized hyperparameters:\", best_params)\n",
    "except NameError:\n",
    "    print(\"Hyperparameter optimization not found. Using default parameters.\")\n",
    "    best_params = {\n",
    "        \"learning_rate\": 3e-5,\n",
    "        \"batch_size\": 16, \n",
    "        \"epochs\": 3,\n",
    "        \"weight_decay\": 0.1\n",
    "    }\n",
    "    print(\"Default hyperparameters:\", best_params)\n",
    "\n",
    "final_args = TrainingArguments(\n",
    "    output_dir='best_model/',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_params[\"batch_size\"],\n",
    "    num_train_epochs=best_params[\"epochs\"],\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "final_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3).to(device)\n",
    "\n",
    "final_trainer = Trainer(\n",
    "    model=final_model,\n",
    "    args=final_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Training final model...\")\n",
    "final_trainer.train()\n",
    "final_metrics = final_trainer.evaluate()\n",
    "print(f\"Final model metrics: {final_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Model Interpretability with SHAP\n",
    "print(\"Generating SHAP values for model interpretability...\")\n",
    "\n",
    "# Create a function to get model predictions\n",
    "def model_predict(texts):\n",
    "    # Tokenize the input texts\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = final_model(**inputs)\n",
    "    # Return logits\n",
    "    return outputs.logits.cpu().numpy()\n",
    "\n",
    "# Select a subset of validation data for SHAP analysis (for efficiency)\n",
    "shap_examples = X_val.sample(100).tolist()\n",
    "\n",
    "# Initialize SHAP explainer\n",
    "explainer = shap.Explainer(model_predict, tokenizer)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer(shap_examples)\n",
    "\n",
    "# Visualize SHAP values for a few examples\n",
    "plt.figure(figsize=(20, 6))\n",
    "shap.plots.text(shap_values[:10], display=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_values_text.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Advanced Evaluation\n",
    "print(\"\\n8. Advanced Evaluation\")\n",
    "\n",
    "# Generate predictions for the validation set\n",
    "val_preds = final_trainer.predict(val_dataset)\n",
    "val_pred_labels = np.argmax(val_preds.predictions, axis=-1)\n",
    "val_true_labels = y_val.tolist()\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(val_true_labels, val_pred_labels)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "            yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Error Analysis\n",
    "print(\"\\n9. Error Analysis\")\n",
    "\n",
    "# Find misclassified examples\n",
    "misclassified_indices = np.where(val_pred_labels != val_true_labels)[0]\n",
    "misclassified_texts = X_val.iloc[misclassified_indices].tolist()\n",
    "misclassified_true = [val_true_labels[i] for i in misclassified_indices]\n",
    "misclassified_pred = [val_pred_labels[i] for i in misclassified_indices]\n",
    "\n",
    "# Show some misclassified examples\n",
    "print(\"\\nMisclassified Examples:\")\n",
    "for i in range(min(5, len(misclassified_texts))):\n",
    "    print(f\"Text: {misclassified_texts[i]}\")\n",
    "    print(f\"True: {['Negative', 'Neutral', 'Positive'][misclassified_true[i]]}\")\n",
    "    print(f\"Predicted: {['Negative', 'Neutral', 'Positive'][misclassified_pred[i]]}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# Analyze common patterns in misclassifications\n",
    "print(\"\\nAnalyzing common patterns in misclassifications...\")\n",
    "# Length analysis\n",
    "misclassified_lengths = [len(text.split()) for text in misclassified_texts]\n",
    "correctly_classified_indices = np.where(val_pred_labels == val_true_labels)[0]\n",
    "correctly_classified_texts = X_val.iloc[correctly_classified_indices].tolist()\n",
    "correctly_classified_lengths = [len(text.split()) for text in correctly_classified_texts]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist([misclassified_lengths, correctly_classified_lengths], bins=20, \n",
    "         alpha=0.7, label=['Misclassified', 'Correctly Classified'])\n",
    "plt.xlabel('Text Length (words)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Text Length for Misclassified vs. Correctly Classified Examples')\n",
    "plt.legend()\n",
    "plt.savefig('error_analysis_length.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Model Comparison\n",
    "print(\"\\n10. Model Comparison\")\n",
    "\n",
    "# Train a simpler baseline model for comparison\n",
    "# Using a simple TF-IDF + Logistic Regression approach\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"Training baseline model (TF-IDF + Logistic Regression)...\")\n",
    "baseline_model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "baseline_model.fit(X_train, y_train)\n",
    "baseline_preds = baseline_model.predict(X_val)\n",
    "\n",
    "# Compare performance\n",
    "baseline_accuracy = accuracy_score(y_val, baseline_preds)\n",
    "bert_accuracy = final_metrics['eval_accuracy']\n",
    "\n",
    "print(f\"Baseline model accuracy: {baseline_accuracy:.4f}\")\n",
    "print(f\"BERT model accuracy: {bert_accuracy:.4f}\")\n",
    "print(f\"Improvement: {(bert_accuracy - baseline_accuracy) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Time Series Analysis\n",
    "print(\"\\n11. Time Series Analysis\")\n",
    "# Check if the dataframe has a timestamp column\n",
    "if 'tweet_created' in df.columns:\n",
    "    # Convert to datetime if needed\n",
    "    df['tweet_created'] = pd.to_datetime(df['tweet_created'])\n",
    "    \n",
    "    # Aggregate sentiments by date\n",
    "    df['date'] = df['tweet_created'].dt.date\n",
    "    sentiment_by_date = df.groupby('date')['sentiment_label'].value_counts().unstack().fillna(0)\n",
    "    \n",
    "    # Plot sentiment trends over time\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sentiment_by_date.plot(kind='line', ax=plt.gca())\n",
    "    plt.title('Sentiment Trends Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Tweets')\n",
    "    plt.legend(['Negative', 'Neutral', 'Positive'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sentiment_time_series.png')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No timestamp column available for time series analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Save models\n",
    "print(\"\\n12. Saving Models\")\n",
    "# Save the final BERT model\n",
    "final_trainer.save_model(\"final_sentiment_model\")\n",
    "\n",
    "# Save the baseline model\n",
    "import pickle\n",
    "with open(\"baseline_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(baseline_model, f)\n",
    "\n",
    "print(\"Models saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Advanced Monitoring Setup\n",
    "print(\"\\n13. Advanced Monitoring Setup\")\n",
    "print(\"Setting up inference monitoring to track model performance...\")\n",
    "\n",
    "# Create a function to simulate monitoring the model in production\n",
    "def monitor_inference(model, text, true_label=None):\n",
    "    \"\"\"Simulate model monitoring in production\"\"\"\n",
    "    # Record the inference time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get prediction\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    \n",
    "    # Record the time taken\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # If true label is provided, record accuracy\n",
    "    accuracy = None\n",
    "    if true_label is not None:\n",
    "        accuracy = 1 if prediction == true_label else 0\n",
    "    \n",
    "    return {\n",
    "        \"prediction\": prediction,\n",
    "        \"inference_time\": inference_time,\n",
    "        \"accuracy\": accuracy\n",
    "    }\n",
    "\n",
    "# Demonstrate monitoring on a few examples\n",
    "monitoring_examples = X_val.iloc[:5].tolist()\n",
    "monitoring_labels = y_val.iloc[:5].tolist()\n",
    "\n",
    "for i, (text, label) in enumerate(zip(monitoring_examples, monitoring_labels)):\n",
    "    result = monitor_inference(final_model, text, label)\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Prediction: {['Negative', 'Neutral', 'Positive'][result['prediction']]}\")\n",
    "    print(f\"  True label: {['Negative', 'Neutral', 'Positive'][label]}\")\n",
    "    print(f\"  Inference time: {result['inference_time']*1000:.2f} ms\")\n",
    "    print(f\"  Correct: {'Yes' if result['accuracy'] == 1 else 'No'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Cost Analysis\n",
    "print(\"\\n14. Cost Analysis\")\n",
    "print(\"Performing cost analysis for model deployment...\")\n",
    "\n",
    "# Simulate cost analysis for model deployment\n",
    "def estimate_cost(model_size_mb, requests_per_day, cost_per_inference=0.0001, cost_per_gb_hour=0.5, hours_per_day=24):\n",
    "    \"\"\"Estimate cost for model deployment\"\"\"\n",
    "    # Calculate inference cost per day\n",
    "    daily_inference_cost = requests_per_day * cost_per_inference\n",
    "    \n",
    "    # Calculate storage and compute cost\n",
    "    storage_cost = (model_size_mb / 1000) * cost_per_gb_hour * hours_per_day\n",
    "    \n",
    "    # Total cost per day\n",
    "    total_daily_cost = daily_inference_cost + storage_cost\n",
    "    \n",
    "    # Monthly cost\n",
    "    monthly_cost = total_daily_cost * 30\n",
    "    \n",
    "    return {\n",
    "        \"daily_inference_cost\": daily_inference_cost,\n",
    "        \"daily_storage_cost\": storage_cost,\n",
    "        \"total_daily_cost\": total_daily_cost,\n",
    "        \"total_monthly_cost\": monthly_cost\n",
    "    }\n",
    "\n",
    "# Estimate model size\n",
    "model_size_mb = 500  # Approximate size of BERT base model in MB\n",
    "requests_per_day = 10000  # Hypothetical number of requests per day\n",
    "\n",
    "cost_estimate = estimate_cost(model_size_mb, requests_per_day)\n",
    "\n",
    "print(f\"Daily inference cost: ${cost_estimate['daily_inference_cost']:.2f}\")\n",
    "print(f\"Daily storage cost: ${cost_estimate['daily_storage_cost']:.2f}\")\n",
    "print(f\"Total daily cost: ${cost_estimate['total_daily_cost']:.2f}\")\n",
    "print(f\"Total monthly cost: ${cost_estimate['total_monthly_cost']:.2f}\")\n",
    "\n",
    "# 15. Documentation and Reporting\n",
    "print(\"\\n15. Documentation and Reporting\")\n",
    "print(\"Generating project documentation and report...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a simple Markdown report\n",
    "report = \"\"\"\n",
    "# Sentiment Analysis Project Report\n",
    "\n",
    "## Project Overview\n",
    "This project implements advanced sentiment analysis using BERT and traditional ML models.\n",
    "\n",
    "## Data Analysis\n",
    "- Dataset size: {data_size} tweets\n",
    "- Class distribution: {class_distribution}\n",
    "\n",
    "## Model Performance\n",
    "- BERT model accuracy: {bert_accuracy:.4f}\n",
    "- Baseline model accuracy: {baseline_accuracy:.4f}\n",
    "- Performance improvement: {improvement:.2f}%\n",
    "\n",
    "## Key Findings\n",
    "- The BERT model significantly outperforms the baseline model\n",
    "- Misclassifications tend to happen more with {misclass_trend}\n",
    "- Sentiment trends show {sentiment_trend}\n",
    "\n",
    "## Deployment Considerations\n",
    "- Estimated monthly cost: ${monthly_cost:.2f}\n",
    "- Average inference time: {avg_inference_time:.2f} ms\n",
    "\n",
    "## Recommendations\n",
    "1. Consider fine-tuning the model further with additional data\n",
    "2. Implement a monitoring system to detect performance degradation\n",
    "3. Regular retraining to adapt to changing language patterns\n",
    "\"\"\".format(\n",
    "    data_size=len(df),\n",
    "    class_distribution=df['sentiment_label'].value_counts().to_dict(),\n",
    "    bert_accuracy=bert_accuracy,\n",
    "    baseline_accuracy=baseline_accuracy,\n",
    "    improvement=(bert_accuracy - baseline_accuracy) * 100,\n",
    "    misclass_trend=\"shorter texts\" if np.mean(misclassified_lengths) < np.mean(correctly_classified_lengths) else \"longer texts\",\n",
    "    sentiment_trend=\"variations over time\" if 'tweet_created' in df.columns else \"not analyzed (no time data)\",\n",
    "    monthly_cost=cost_estimate['total_monthly_cost'],\n",
    "    avg_inference_time=np.mean([monitor_inference(final_model, text)['inference_time'] * 1000 for text in X_val.iloc[:10].tolist()])\n",
    ")\n",
    "\n",
    "# Save the report to a file\n",
    "with open(\"sentiment_analysis_report.md\", \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"Report saved to sentiment_analysis_report.md\")\n",
    "print(\"\\nProject complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
